<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Python Code Display</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            background-color: #f4f4f4;
            margin: 20px;
            padding: 20px;
        }
        pre {
            background-color: #333;
            color: #fff;
            padding: 20px;
            border-radius: 5px;
            overflow-x: auto;
            white-space: pre-wrap;
            word-wrap: break-word;
        }
    </style>
</head>
<body>
    <h2>Python Code</h2>
    <pre>
Practical 2
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
%matplotlib inline

cust = pd.read_csv("/content/Ecommerce Customers.csv")
cust.head()
cust.describe()
cust.info()
import seaborn as sns
sns.jointplot(x='Time on Website',y='Yearly Amount Spent',data=cust)

sns.jointplot(x='Time on App',y='Yearly Amount Spent',data=cust)

sns.jointplot(x='Time on App',y='Yearly Amount Spent',data=cust,kind='kde')

sns.pairplot(cust)

y=cust['Yearly Amount Spent']
x=cust[['Avg. Session Length','Time on App','Time on Website','Length of Membership']]
from sklearn.model_selection import train_test_split
x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.3,random_state=129)
x_train
x_test
y_train
y_test
from sklearn.linear_model import LinearRegression
lm = LinearRegression()
lm.fit(x_train,y_train)
print('Coefficients: \n',lm.coef_)
print(lm.intercept_)
pred=lm.predict(x_test)
pred
plt.scatter(y_test,pred)
plt.xlabel('Y Test')
plt.ylabel('Predicted Y')

from sklearn import metrics
print('MAE:',metrics.mean_absolute_error(y_test,pred))
print('MSE:',metrics.mean_squared_error(y_test,pred))
print('RMSE:',np.sqrt(metrics.mean_squared_error(y_test,pred)))
sns.distplot((y_test-pred),bins=50)

coe = pd.DataFrame(lm.coef_,x.columns,columns=['Coeff'])
coe

********************************************************************************************************************************************************************

practical 3

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression, Lasso, Ridge
from sklearn.metrics import mean_squared_error
from sklearn.preprocessing import PolynomialFeatures

# Load data
df = pd.read_csv("Ecommerce Customers - Ecommerce Customers.csv")

# Define features (X) and target (y)
x = df[['Time on Website']]
y = df[['Yearly Amount Spent']]

# Split data into training and testing sets
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)

# Polynomial Regression
degree = 2
poly_reg = PolynomialFeatures(degree=degree, include_bias=False)
x_poly = poly_reg.fit_transform(x_train)
ploy_reg = LinearRegression().fit(x_poly, y_train)
x_test_poly = poly_reg.transform(x_test)
poly_pred = ploy_reg.predict(x_test_poly)

# Linear Regression
linear_reg = LinearRegression().fit(x_train, y_train)
linear_pred = linear_reg.predict(x_test)

# Lasso Regression
alpha_lasso = 0.1
lasso_reg = Lasso(alpha=alpha_lasso).fit(x_train, y_train)
lasso_pred = lasso_reg.predict(x_test)

# Ridge Regression
alpha_ridge = 0.1
ridge_reg = Ridge(alpha=alpha_ridge).fit(x_train, y_train)
ridge_pred = ridge_reg.predict(x_test)

# Print Mean Squared Errors
print("Linear Regression MSE:", mean_squared_error(y_test, linear_pred))
print(f"Polynomial Regression (Degree: {degree}) MSE:", mean_squared_error(y_test, poly_pred))
print(f"Lasso Regression (Alpha: {alpha_lasso}) MSE:", mean_squared_error(y_test, lasso_pred))
print(f"Ridge Regression (Alpha: {alpha_ridge}) MSE:", mean_squared_error(y_test, ridge_pred))

# Plotting functions
def plot_regression(title, y_pred, color, label):
    plt.scatter(x_test, y_test, color=color, label='Actual Data')
    plt.plot(x_test, y_pred, label=label)
    plt.legend()
    plt.xlabel('Time on Website')
    plt.ylabel('Yearly Amount Spent')
    plt.title(title)
    plt.show()

# Plot comparisons
plot_regression('Linear Regression', linear_pred, 'blue', 'Linear Regression')
plot_regression(f'Polynomial Regression (Degree: {degree})', poly_pred, 'red', f'Polynomial Regression (Degree: {degree})')
plot_regression(f'Lasso Regression (Alpha {alpha_lasso})', lasso_pred, 'yellow', f'Lasso Regression (Alpha {alpha_lasso})')
plot_regression(f'Ridge Regression (Alpha {alpha_ridge})', ridge_pred, 'pink', f'Ridge Regression (Alpha {alpha_ridge})')

********************************************************************************************************************************************************************

Practical 4

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.svm import SVC
from sklearn import metrics

# Load the dataset
df = pd.read_csv("iphone_purchase_records.csv")

# Add log-transformed columns for 'Age' and 'Salary'
df['log_age'] = np.log(df['Age'])
df['log_sal'] = np.log(df['Salary'])
df.head()

# Visualize the data
plt.figure(figsize=(10, 4))

plt.subplot(1, 3, 1)
sns.swarmplot(y='Salary', x='Purchase Iphone', hue='Gender', data=df)
plt.title("Salary Distribution by Purchase and Gender")

plt.subplot(1, 3, 2)
sns.countplot(x='Gender', hue='Purchase Iphone', data=df)
plt.title("Purchase Count by Gender")

plt.subplot(1, 3, 3)
sns.scatterplot(x='Age', y='Salary', hue='Purchase Iphone', data=df)
plt.title("Age vs Salary by Purchase")

plt.tight_layout()
plt.show()

# Prepare features (X) and target (y)
X = np.array(df[['Gender', 'log_age', 'log_sal']])
y = np.array(df[['Purchase Iphone']])

# Encode 'Gender' column
labelEncoder_gender = LabelEncoder()
X[:, 0] = labelEncoder_gender.fit_transform(X[:, 0])

# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=0)

# Standardize features
sc = StandardScaler()
X_train = sc.fit_transform(X_train)
X_test = sc.transform(X_test)  # Use 'transform' instead of 'fit_transform' here

# Train the SVM classifier
classifier = SVC(kernel='linear', random_state=0)
classifier.fit(X_train, y_train.ravel())  # Flatten y_train to 1D array

# Predict on the test set
y_pred = classifier.predict(X_test)

# Evaluate the model
cm = metrics.confusion_matrix(y_test, y_pred)
accuracy = metrics.accuracy_score(y_test, y_pred)
precision = metrics.precision_score(y_test, y_pred)
recall = metrics.recall_score(y_test, y_pred)

print("Confusion Matrix:\n", cm)
print("Accuracy score:", accuracy)
print("Precision score:", precision)
print("Recall score:", recall)

********************************************************************************************************************************************************************

Practical 5

from sklearn import datasets
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import confusion_matrix,classification_report

iris=datasets.load_iris()
irs=pd.DataFrame(iris.data,columns=iris.feature_names)
irs['class']=iris.target
#print(irs)
#print(irs.describe())
x=irs.iloc[:,:-1].values
y=irs.iloc[:,4].values

x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.20)

scaler=StandardScaler()
scaler.fit(x_train)
x_train=scaler.transform(x_train)
x_test=scaler.transform(x_test)
classifier=KNeighborsClassifier(n_neighbors=5)
classifier.fit(x_train,y_train)

y_pred=classifier.predict(x_test)

print(confusion_matrix(y_test,y_pred))
print(classification_report(y_test,y_pred))

********************************************************************************************************************************************************************
Practical 6

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
# %matplotlib inline

import pandas as pd
data = pd.read_csv("/content/mail.csv")

data.head()

data[['Age', 'Annual Income (k$)', 'Spending Score (1-100)']].corr()

plt.figure(figsize=(10,6))
sns.set(style='whitegrid')
sns.distplot(data['Annual Income (k$)'])
plt.title('Distribution of Annual Income (k$) :- 9130 ',fontsize=20)
plt.xlabel('Range Annual Income (k$)')
plt.ylabel('count')

plt.figure(figsize=(10,6))
sns.set(style='whitegrid')
sns.displot(data['Age'])
plt.title('Distribution of Age :- 9130 ',fontsize=20)
plt.xlabel('Range Age')
plt.ylabel('count')

plt.figure(figsize=(10,6))
sns.set(style='whitegrid')
sns.distplot(data['Spending Score (1-100)'])
plt.title('Distribution of Spending Score (1-100) :- 9130 ',fontsize=20)
plt.xlabel('Range Spending Score (1-100)')
plt.ylabel('count')

gender = data.Gender.value_counts()
sns.set_style("darkgrid")
plt.figure(figsize=(10,4))
sns.barplot(x=gender.index, y=gender.values)
plt.show()

plt.figure(figsize=(10,6))
sns.scatterplot(x='Age',y='Annual Income (k$)' , hue='Gender',data=data , s=60)
plt.xlabel('Age'),plt.ylabel('Annual Income (k$)')
plt.title('age va Annual Income w.r.t Gender')
plt.legend()
plt.show()

plt.figure(figsize=(10,6))
sns.scatterplot(x='Annual Income (k$)', y='Spending Score (1-100)', hue='Gender', data=data, s=60)
plt.xlabel('Annual Income (k$)'), plt.ylabel('Spending Score (1-100)')
plt.title('Spending Score (1-100) vs Annual income (k$)')
plt.legend()
plt.show()

ss1_20 = data["Spending Score (1-100)"][(data["Spending Score (1-100)"] > 1) & (data["Spending Score (1-100)"] < 20)]
ss21_40 = data["Spending Score (1-100)"][(data["Spending Score (1-100)"] >= 21) & (data["Spending Score (1-100)"] < 40)]
ss41_60 = data["Spending Score (1-100)"][(data["Spending Score (1-100)"] >= 41) & (data["Spending Score (1-100)"] < 60)]
ss61_80 = data["Spending Score (1-100)"][(data["Spending Score (1-100)"] >= 61) & (data["Spending Score (1-100)"] < 80)]
ss81_100 = data["Spending Score (1-100)"][(data["Spending Score (1-100)"] >= 81) & (data["Spending Score (1-100)"] <= 100)]

score_x = ["1-20", "21-40", "41-60", "61-80", "81-100"]
score_y = [len(ss1_20.values), len(ss21_40.values), len(ss41_60.values), len(ss61_80.values), len(ss81_100.values)]

plt.figure(figsize=(10, 6))
sns.barplot(x=score_x, y=score_y, palette="Set2")
plt.title("Spending Scores 9130")
plt.xlabel("Score")
plt.ylabel("Number of Customer Having the Spending Score in that Range")
plt.show()

ss1_20 = data["Spending Score (1-100)"][(data["Spending Score (1-100)"] <= 20)]
ss21_40 = data["Spending Score (1-100)"][(data["Spending Score (1-100)"] > 21) & (data["Spending Score (1-100)"] <= 40)]
ss41_60 = data["Spending Score (1-100)"][(data["Spending Score (1-100)"] > 41) & (data["Spending Score (1-100)"] <= 60)]
ss61_80 = data["Spending Score (1-100)"][(data["Spending Score (1-100)"] > 61) & (data["Spending Score (1-100)"] <= 80)]
ss81_100 = data["Spending Score (1-100)"][(data["Spending Score (1-100)"] > 81) & (data["Spending Score (1-100)"] <= 100)]
score = ["1-20", "21-40", "41-60", "61-80", "81-100"]
score_y = [len(ss1_20.values), len(ss21_40.values), len(ss41_60.values), len(ss61_80.values), len(ss81_100.values)]
plt.figure(figsize=(10, 6))
sns.barplot(x=score, y=score_y, palette="Set2")
plt.title("Spending Scores")
plt.xlabel("Score")
plt.ylabel("Number of Customer Having the Spending Score In That Range")
plt.show()

a10_30 = data["Annual Income (k$)"] [(data["Annual Income (k$)"] >= 0) & (data["Annual Income (k$)"] < 30)]
a131_60 = data["Annual Income (k$)"] [(data["Annual Income (k$)"] >= 31) & (data["Annual Income (k$)"] < 60)]
a161_90 = data["Annual Income (k$)"] [(data["Annual Income (k$)"] >= 61) & (data["Annual Income (k$)"] < 90)]
a191_120 = data["Annual Income (k$)"] [(data["Annual Income (k$)"] >= 91) & (data["Annual Income (k$)"] < 120)]
a121_150 = data["Annual Income (k$)"] [(data["Annual Income (k$)"] >= 121) & (data["Annual Income (k$)"] < 150)]

income_x = ["$ 0 - 30,000", "$ 30,001 - 50,000", "$ 50,001 - 90,000", "$ 90,001 - 120,000", "$ 120,001 - 150,000"]
income_y = [len(a10_30.values), len(a131_60.values), len(a161_90.values), len(a191_120.values), len(a121_150.values)]

plt.figure(figsize=(15, 6))
sns.barplot(x=income_x, y=income_y, palette="nipy_spectral")
plt.title("Annual Incomes")
plt.xlabel("Income")
plt.ylabel("Number of Customers")
plt.show()

data

df1 = data[["CustomerID", "Gender", "Age", "Annual Income (k$)", "Spending Score (1-100)"]]  # Corrected column names
x = df1[["Annual Income (k$)", "Spending Score (1-100)"]]

x.head()

plt.figure(figsize=(10,6))
sns.scatterplot(x='Annual Income (k$)', y='Spending Score (1-100)', data= x ,s=60)
plt.xlabel('Annual Income (k$)'), plt.ylabel('Spending Score (1-100)')
plt.title('Spending Score (1-100) vs Annual income (k$)')
plt.show()

from sklearn.cluster import KMeans

wcss = []
for i in range(1,11):
  km = KMeans(n_clusters=1)
  km.fit(x)
  wcss.append(km.inertia_)

plt.figure(figsize =(12,6))
plt.plot(range(1,11),wcss)
plt.plot(range(1,11),wcss, linewidth = 2, color = "red", marker ="8")
plt.xlabel("K Value")
plt.xticks(np.arange(1,11,1))
plt.ylabel("WCSS")
plt.show()

km1=KMeans(n_clusters=5)
km1.fit(x)
y=km1.predict(x)

df1['label'] = y
df1.head()

plt.figure(figsize=(10,6))
sns.scatterplot(x='Annual Income (k$)', y='Spending Score (1-100)', hue='label',
                palette=['green','orange','brown','dodgerblue','red'],data=df1, s=60)
plt.xlabel('Annual Income (k$)'), plt.ylabel('Spending Score (1-100)')
plt.xlabel('Annual Income (k$)'), plt.ylabel('Spending Score (1-100)')
plt.title('Spending Score (1-100) vs Annual income (k$)')
plt.show()

cust0=df1[df1["label"]==0]
print('Number of customer in 0th group=', len(cust0))
print('They are -', cust0["CustomerID"].values)
print("--------------------------------------------------")
cust1=df1[df1["label"]==1]
print('Number of customer in 1st group=', len(cust1))
print('They are -', cust1["CustomerID"].values)
print("--------------------------------------------------")
cust2=df1[df1["label"]==2]
print('Number of customer in 2nd group=', len(cust2))
print('They are -', cust2["CustomerID"].values)
print("--------------------------------------------------")
cust3=df1[df1["label"]==3]
print('Number of customer in 3rd group=', len(cust3))
print('They are -', cust3["CustomerID"].values)
print("--------------------------------------------------")
cust4=df1[df1["label"]==4]
print('Number of customer in 4th group=', len(cust4))
print('They are -', cust4["CustomerID"].values)

********************************************************************************************************************************************************************

Practical 7

%matplotlib inline
import matplotlib.pyplot as plt
import numpy as np
X=np.array([
    [7,8],[12,20],[17,23],[26,15],[32,37],[87,75],[73,85],[62,80],[73,60],[87,96]
])
labels=range(1,11)
plt.figure(figsize=(10,7))
plt.scatter(X[:,0],X[:,1],label='True Position')
for label,x,y in zip(labels,X[:,0],X[:,1]):
  plt.annotate(label,xy=(x,y),xytext=(-3,3),textcoords='offset points',ha='right',va='bottom')

plt.show()
from scipy.cluster.hierarchy import dendrogram,linkage
from matplotlib import pyplot as plt
linked=linkage(X,'average')
labelList=range(1,11)
plt.figure(figsize=(5,5))
dendrogram(linked,orientation='top',labels=labelList,
          distance_sort='descending',show_leaf_counts=True)

plt.show()
from sklearn.cluster import AgglomerativeClustering
cluster=AgglomerativeClustering(n_clusters=3,linkage='ward')
cluster.fit_predict(X)
np.random.seed(0)
X = np.random.randint(1, 100, size=(10, 2))
labels=range(1,11)
plt.figure(figsize=(10,7))
plt.scatter(X[:,0],X[:,1],label='True Position')
for label,x,y in zip(labels,X[:,0],X[:,1]):
  plt.annotate(label,xy=(x,y),xytext=(-3,3),textcoords='offset points',ha='right',va='bottom')
plt.show()
from scipy.cluster.hierarchy import dendrogram,linkage
from matplotlib import pyplot as plt
linked=linkage(X,'average')
labelList=range(1,11)
plt.figure(figsize=(5,5))
dendrogram(linked,orientation='top',labels=labelList,
          distance_sort='descending',show_leaf_counts=True)
plt.show()
from sklearn.cluster import AgglomerativeClustering
cluster=AgglomerativeClustering(n_clusters=3,linkage='ward')
cluster.fit_predict(X)

********************************************************************************************************************************************************************
practical 8
        
!pip install apyori

import pandas as pd
import numpy as np
from apyori import apriori
path = '/content/Market_Basket_Optimisation.csv'
df=pd.read_csv(path)
df.fillna(0,inplace=True)
df.head()

transactions=[]
for i in range(0,len(df)):
  transactions.append([str(df.values[i,j])for j in range(0,20) if str(df.values[i,j])!='0'])

transactions[0]

transactions[1]

rules=apriori(transactions,min_support=0.003,min_confidence=0.2,min_lift=3,min_length=2)
rules

Results=list(rules)
Results

df_results=pd.DataFrame(Results)
df_results.head()

support=df_results.support

first_values=[]
second_values=[]
third_values=[]
fourth_values=[]

for i in range(df_results.shape[0]):
  single_list=df_results['ordered_statistics'][i][0]
  first_values.append(list(single_list[0]))
  second_values.append(list(single_list[1]))
  third_values.append(single_list[2])
  fourth_values.append(single_list[3])

lhs=pd.DataFrame(first_values)
rhs=pd.DataFrame(second_values)
confidence=pd.DataFrame(third_values,columns=['Confidence'])
lift=pd.DataFrame(fourth_values,columns=['lift'])

df_final=pd.concat([lhs,rhs,support,confidence,lift],axis=1)
df_final

df_final.fillna(value=' ',inplace=True)
df_final.head()

df_final.columns=['lhs',1,'rhs',2,3,4,'support','confidence','lift']
df_final.head()

df_final['lhs']=df_final['lhs']+str(", ")+df_final[1]
df_final['rhs']=df_final['rhs']+str(", ")+df_final[2]+str(", ")+df_final[3]
df_final.head()

df_final.drop(columns=[1,2,3,4],inplace=True)
df_final.head()

df_final.sort_values('lift',ascending=False).head(10)


____________________________________________________________________________________________________________________________________________________________________
practical 9

# Commented out IPython magic to ensure Python compatibility.
import warnings
import matplotlib.pyplot as plt
import seaborn as sns
from scipy.stats import pearsonr
from sklearn.naive_bayes import GaussianNB
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, recall_score, precision_score
# %matplotlib inline
warnings.filterwarnings("ignore")
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split

def cross_validate(estimator,train,validation):
  X_train=train[0]
  Y_train=train[1]
  X_val=validation[0]
  Y_val=validation[1]
  train_predictions=classifier.predict(X_train)
  train_accuracy=accuracy_score(Y_train,train_predictions)
  train_recall=recall_score(Y_train,train_predictions)
  train_precision=precision_score(Y_train,train_predictions)
  val_predictions=classifier.predict(X_val)
  val_accuracy=accuracy_score(Y_val,val_predictions)
  val_recall=recall_score(Y_val,val_predictions)
  val_precision=precision_score(Y_val,val_predictions)

  print('Model metrics')
  print('Accuracy Train: %.2f, Validation: %.2f' %(train_accuracy,val_accuracy))
  print('Recall Train: %.2f, Validation: %.2f' % (train_recall,val_recall))
  print('Precision Train: %.2f, Validation: %.2f' % (train_precision,val_precision))

test_raw=pd.read_csv('/content/test.csv')
train_raw=pd.read_csv('/content/train (1).csv')

test_ids=test_raw['PassengerId'].values
train_raw['train']=1
test_raw['train']=0
data = pd.concat([test_raw, train_raw], sort=False)
data.head()

data.describe()

features=['Age','Embarked','Fare','Parch','Pclass','Sex','SibSp']
target='Survived'
data=data[features+[target]+['train']]
data['Sex']=data['Sex'].replace(["female","male"],[0,1])
data['Embarked']=data['Embarked'].replace(['S','C','Q'],[1,2,3])
data['Age']=pd.qcut(data['Age'],10,labels=False)

train=data.query('train==1')
test=data.query('train==0')
train.dropna(axis=0,inplace=True)
labels=train[target].values
train.head()

columns=train[features+[target]].columns.tolist()
nColumns=len(columns)
result=pd.DataFrame(np.zeros((nColumns,nColumns)),columns=columns)

for col_a in range(nColumns):
  for col_b in range(nColumns):
    result.iloc[[col_a,col_b]]=pearsonr(train.loc[:,columns[col_a]],train.loc[:,columns[col_b]])[0]

fig,ax=plt.subplots(figsize=(10,10))
ax=sns.heatmap(result,yticklabels=columns,vmin=1,vmax=1,annot=True,fmt='.2f',linewidths=.2)
ax.set_title('Pearson Correlation')
plt.show()

continuous_numeric_features=['Age','Fare','Parch','SibSp']
for feature in continuous_numeric_features:
  sns.distplot(train[features])
  plt.show()

train.drop(['train',target,'Pclass'],axis=1,inplace=True)
test.drop(['train',target,'Pclass'],axis=1,inplace=True)
X_train,X_val,Y_train,Y_val=train_test_split(train,labels,test_size=0.2,random_state=29)
X_train.head()

X_train1,X_train2,Y_train1,Y_train2=train_test_split(train,labels,test_size=0.3,random_state=29)
classifier=GaussianNB()
classifier.fit(X_train2,Y_train2)
print('Metrics with only 30% of train data')
cross_validate(classifier,(X_train,Y_train),(X_val,Y_val))

classifier.partial_fit(X_train1,Y_train1)
print('Metrics with 70% of train data')
cross_validate(classifier,(X_train,Y_train),(X_val,Y_val))

print('Probability of each class')
print('Survive = 0: %.2f' % classifier.class_prior_[0])
print('Survive = 1: %.2f' % classifier.class_prior_[1])

print('Mean of each feature per class')
print('                 Age       Embarked      Fare        Parch       Sex       SibSp')
print('Survive = 0: %s' % classifier.theta_[0])
print('Survive = 1: %s' % classifier.theta_[1])

print('Variance of each feature per class')
print('Survive = 0: %s' % classifier.var_[0])
print('Survive = 1: %s' % classifier.var_[1])

test.fillna(test.mean(),inplace=True)
test_predictions=classifier.predict(test)
submission=pd.DataFrame({'PassengerId': test_ids})
submission['Survived']=test_predictions.astype('int')
submission.to_csv('submission.csv',index=False)
submission.head(10)

********************************************************************************************************************************************************************

Practical 10

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
df = pd.read_csv('/content/heart.csv')
df.head()
x=df.drop("target",axis=1)
y=df["target"]
from sklearn.ensemble import RandomForestClassifier
clf = RandomForestClassifier()
clf.get_params()
x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.2)
clf.fit(x_train,y_train)
y_preds=clf.predict(x_test)
y_preds
y_test
clf.score(x_train,y_train)
clf.score(x_test,y_test)
from sklearn.metrics import classification_report,confusion_matrix,accuracy_score
print(classification_report(y_test,y_preds))
confusion_matrix(y_test,y_preds)
accuracy_score(y_test,y_preds)
np.random.seed(42)
for i in range(1,22,1):
  print(f"Trying model with {i} estimators")
  clf=RandomForestClassifier(n_estimators=i).fit(x_train,y_train)
  print(f"Model accuracy on test set: {clf.score(x_test,y_test)*100:.2f}%")
  print("")

    </pre>
</body>
</html>
